{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom random import random\nfrom random import seed\nfrom math import exp\nimport pickle\n\n#Initializing a Neural Network\n\nweight_NeuralNetwork = []\ninputsEntered_NeuralNetwork = 1\nbias = 1\nnew_inputsEntered_NeuralNetwork = []\n\ndef neuralNetwork_instantiation(neuralNetwork_inputsEntered, neuralNetwork_encapsulatedLayerGenerated, neuralNetwork_outputLayerGenerated):\n    neuralNetwork = list()\n    encapsulatedLayerGenerated = [{'weights':[random() for i in range(neuralNetwork_inputsEntered + 4)]} for i in range(neuralNetwork_encapsulatedLayerGenerated)]\n    neuralNetwork.append(encapsulatedLayerGenerated)\n    outputLayerGenerated = [{'weights':[random() for i in range(neuralNetwork_encapsulatedLayerGenerated + 4)]} for i in range(neuralNetwork_outputLayerGenerated)]\n    neuralNetwork.append(outputLayerGenerated)\n    return neuralNetwork\nseed(2)\nneuralNetwork = neuralNetwork_instantiation(4, 3, 5)\nfor subsection in neuralNetwork:\n    print(subsection)\n\n#Activating a Neuron and Transferring it for Forward Propagation Stage\nactivationLayer = sum(weight_NeuralNetwork * inputsEntered_NeuralNetwork) + bias\ndef neuralNetwork_activate(weight_NeuralNetwork, inputsEntered_NeuralNetwork):\n    activationLayer = inputsEntered_NeuralNetwork[-1]\n    for x in range(len(weight_NeuralNetwork) -1):\n        activationLayer += weight_NeuralNetwork[x] * inputsEntered_NeuralNetwork[x]\n    return activationLayer\ndef transfer(activationLayer):\n    return 1.0 / (1.0 + exp(-activationLayer))\n#Forward Propagation Stage of Neural Network\ndef forwardPropagate_NeuralNetwork(neuralNetwork, row):\n    inputsEntered_NeuralNetwork = row\n    for activationLayer in neuralNetwork:\n        newInputs = []\n        for neuron in activationLayer:\n            activationLayer = neuralNetwork_activate(neuron['weights'], inputsEntered_NeuralNetwork)\n            neuron['output'] = transfer(activationLayer)\n            new_inputsEntered_NeuralNetwork.append(neuron['output'])\n        inputsEntered_NeuralNetwork = new_inputsEntered_NeuralNetwork\n    return inputsEntered_NeuralNetwork\nneuralNetwork = [[{'weights':[0.9560342718892494, 0.9478274870593494, 0.05655136772680869]}], [{'weights':[0.6059441656784624, 0.6068017336408379]}, {'weights':[0.4569832346712365, 0.1654123543123652]}]]\nrow = [3, 1, 4]\noutput = forwardPropagate_NeuralNetwork(neuralNetwork, row)\nprint(output)\n\n#Transfer Derivative with Sigmoid Activation Function\ndef transferDerivative_NeuralNetwork(outputGenerated):\n    return (outputGenerated * 1.0 - outputGenerated)\n\n#Error Backpropagation\ndef backwardPropagateError_NeuralNetwork(outputGenerated, expectedValue):\n    for a in reversed(len(neuralNetwork)):\n                      activationLayer = neuralNetwork[i]\n                      errors = list()\n                      if a!= len(neuralNetwork) - 1:\n                          for b in range(len(activationLayer)):\n                            error = 0.0\n                        for neuron in neuralNetwork[a+1]:\n                                error += (neuron['weights'][b] * neuron['delta'])\n                                errors.append(error)\nerror = (outputGenerated - expectedValue) * transferDerivative_NeuralNetwork(outputGenerated)\n\n                      else:\n                          for b in range(len(activationLayer):\n                            neuron = activationLayer[b]\n                            errors.append(neuron['output'] - expectedValue[b]\n                          for b in range(len(activationLayer):\n                            neuron = activationLayer[b]\n                            neuron['delta'] = errors[b] * transferDerivative_NeuralNetwork(neuron['output'])\n\n# test backpropagation of error\nnetwork = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n           [{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\nexpectedValue = [0, 1]\nbackwardPropagateError_NeuralNetwork(neuralNetwork, expectedValue)\nfor activationLayer in neuralNetwork:\n    print(activationLayer)\n\n#Updating the Weights of the Convolutional Neural Network\ndef updatingWeights_NeuralNetwork(neuralNetwork, row, learningRate):\n    for a in range(len(neuralNetwork)):\n        inputsEntered_NeuralNetwork = row[:-1]\n        if a != 0: \n            inputsEntered_NeuralNetwork = [neuron['output'] for neuron in neuralNetwork[a-1]]\n            for neuron in neuralNetwork[a]:\n                for b in range(len(inputsEntered_NeuralNetwork)):\n                    neuron['weights'][b] = learningRate * neuron['delta'] * inputsEntered_NeuralNetwork[b]\n                    neuron['weights'][-1] = learningRate * neuron['delta']\n#Training a network for fixed number of epochs\n#epoch - training the neural network with all the training data for one cycle, all data is used exactly once\n\nneuralEpoch_neuralNetwork = 1\nlearningRate_neuralNetwork = 1\nneuralOutputsGenerated = 3\nsumError_neuralNetwork = 0\ndef trainNetwork_NeuralNetwork(neuralNetwork, trainingNeuralNetwork, learningRate_neuralNetwork, neuralEpoch_neuralNetwork, neuralOutputsGenerated):\n    for epoch in range (neuralEpoch_neuralNetwork):\n        for row in trainingNeuralNetwork:\n            neuralOutputsGenerated = forwardPropagate_NeuralNetwork(neuralNetwork, row)\n            expectedValue = [0 for i in range(neuralOutputsGenerated)]\n            expectedValue[row[-1]] = 1\n            sumError_neuralNetwork += sum([(expectedValue[a] - neuralOutputsGenerated[a])**2 for a in range(len(expectedValue))])\n            backwardPropagateError_NeuralNetwork(neuralNetwork, expectedValue)\n            updatingWeights_NeuralNetwork(neuralNetwork, row, learningRate)\nprint('>epoch=%d, learningRate=%.3f, error=%.3f'%(neuralEpoch_neuralNetwork, learningRate_neuralNetwork, sumError_neuralNetwork))\ntrainingDataset_neuralNetwork = [[2.7810836,2.550537003,0],\n                                 [1.465489372,2.362125076,0],[3.396561688,4.400293529,0],[1.38807019,1.850220317,0],[3.06407232,3.005305973,0],\n                                 [7.627531214,2.759262235,1],[5.332441248,2.088626775,1],[6.922596716,1.77106367,1],[8.675418651,-0.242068655,1],\n                                 [7.673756466,3.508563011,1]]\nneuralNetwork_inputsEntered = len(trainingDataset_neuralNetwork[0]) - 1\nneuralNetwork_outputLayerGenerated = len(set([row[-1] for row in trainingDataset_neuralNetwork]))\nneuralNetwork = neuralNetwork_instantiation(neuralNetwork_inputsEntered, 3, neuralNetwork_outputLayerGenerated)\ntrainNetwork_NeuralNetwork(neuralNetwork, trainingDataset_neuralNetwork, 1.5, 30, neuralNetwork_outputLayerGenerated)\nfor activationLayer in neuralNetwork:\n    print(activationLayer)\n    activationLayer = pickle.load(trainingDataset_neuralNetwork)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}