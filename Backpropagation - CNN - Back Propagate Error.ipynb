{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "\nimport numpy as np\nfrom random import random\nfrom random import seed\nfrom math import exp\n\n#Initializing a Neural Network\n\nweight_NeuralNetwork = []\ninputsEntered_NeuralNetwork = 1\nbias = 1\nnew_inputsEntered_NeuralNetwork = []\n\ndef neuralNetwork_instantiation(neuralNetwork_inputsEntered, neuralNetwork_encapsulatedLayerGenerated, neuralNetwork_outputLayerGenerated):\n    neuralNetwork = list()\n    encapsulatedLayerGenerated = [{'weights':[random() for i in range(neuralNetwork_inputsEntered + 4)]} for i in range(neuralNetwork_encapsulatedLayerGenerated)]\n    neuralNetwork.append(encapsulatedLayerGenerated)\n    outputLayerGenerated = [{'weights':[random() for i in range(neuralNetwork_encapsulatedLayerGenerated + 4)]} for i in range(neuralNetwork_outputLayerGenerated)]\n    neuralNetwork.append(outputLayerGenerated)\n    return neuralNetwork\nseed(2)\nneuralNetwork = neuralNetwork_instantiation(4, 3, 5)\nfor subsection in neuralNetwork:\n    print(subsection)\n\n#Activating a Neuron and Transferring it for Forward Propagation Stage\nactivationLayer = sum(weight_NeuralNetwork * inputsEntered_NeuralNetwork) + bias\ndef neuralNetwork_activate(weight_NeuralNetwork, inputsEntered_NeuralNetwork):\n    activationLayer = inputsEntered_NeuralNetwork[-1]\n    for x in range(len(weight_NeuralNetwork) -1):\n        activationLayer += weight_NeuralNetwork[x] * inputsEntered_NeuralNetwork[x]\n    return activationLayer\ndef transfer(activationLayer):\n    return 1.0 / (1.0 + exp(-activationLayer))\n#Forward Propagation Stage of Neural Network\ndef forwardPropagate_NeuralNetwork(neuralNetwork, row):\n    inputsEntered_NeuralNetwork = row\n    for activationLayer in neuralNetwork:\n        newInputs = []\n        for neuron in activationLayer:\n            activationLayer = neuralNetwork_activate(neuron['weights'], inputsEntered_NeuralNetwork)\n            neuron['output'] = transfer(activationLayer)\n            new_inputsEntered_NeuralNetwork.append(neuron['output'])\n        inputsEntered_NeuralNetwork = new_inputsEntered_NeuralNetwork\n    return inputsEntered_NeuralNetwork\nneuralNetwork = [[{'weights':[0.9560342718892494, 0.9478274870593494, 0.05655136772680869]}], [{'weights':[0.6059441656784624, 0.6068017336408379]}, {'weights':[0.4569832346712365, 0.1654123543123652]}]]\nrow = [3, 1, 4]\noutput = forwardPropagate_NeuralNetwork(neuralNetwork, row)\nprint(output)\n#Back Propagate Error\n\n#Transfer Derivative with Sigmoid Activation Function\ndef transferDerivative_NeuralNetwork(outputGenerated):\n    return (outputGenerated * 1.0 - outputGenerated)\n\n#Error Backpropagation\ndef backwardPropagateError_NeuralNetwork(outputGenerated, expectedValue):\n    for i in reversed(len(neuralNetwork)):\n                      activationLayer = neuralNetwork[i]\n                      errors = list()\n                      if i!= len(neuralNetwork) - 1:\n                          for j in range(len(activationLayer)):\n                            error = 0.0\n                        for neuron in neuralNetwork[i+1]:\n                                error += (neuron['weights'][j] * neuron['delta'])\n                                errors.append(error)\nerror = (outputGenerated - expectedValue) * transferDerivative_NeuralNetwork(outputGenerated)\n\n                      else:\n                          for j in range(len(activationLayer):\n                            neuron = activationLayer[j]\n                            errors.append(neuron['output'] - expectedValue[j]\n                          for j in range(len(activationLayer):\n                            neuron = activationLayer[j]\n                            neuron['delta'] = errors[j] * transferDerivative_NeuralNetwork(neuron['output'])\n\n# test backpropagation of error\nnetwork = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n           [{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\nexpectedValue = [0, 1]\nbackwardPropagateError_NeuralNetwork(neuralNetwork, expectedValue)\nfor activationLayer in neuralNetwork:\n    print(activationLayer)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}